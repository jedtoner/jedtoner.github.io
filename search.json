[
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "üìç Melbourne, Victoria\n‚úâÔ∏è JedTo.13@gmail.com\nüîó LinkedIn | GitHub\n\n\n\nData Scientist with expertise in Bayesian modeling, forecasting, ML pipelines, and cloud-based solutions.\nSkilled in delivering scalable models and data-driven insights across fast-paced startup and academic environments.\n\n\n\n\n\n\nJan 2024 ‚Äì Present\nData Science\n- Designed and implemented a commercial Marketing Mix Model in Python (PyMC), applying Hierarchical Bayes with partial pooling, nonlinear regression, and advanced adstock/saturation dynamics. Achieved out-of-sample MAPE as low as 11%.\n- Engineered a scalable PCA framework in R to condense a library of 80,000+ macroeconomic variables into actionable inputs.\n- Developed robust time series forecasting pipelines (ARIMA, VAR, LSTM/RNN) with synthetic data generation to ensure model continuity despite data delays.\nMachine Learning Operations (MLOps)\n- Built and maintained API servers, fully containerized with Docker, and implemented CI/CD pipelines via GitHub Actions, reducing deployment time by 80%.\n- Containerized and deployed multiple ML model interfaces to Google Cloud Run for scalable production.\n- Created interactive model evaluation frameworks (Python + HTML), streamlining model selection and benchmarking.\nData Engineering / Other\n- Built and managed end-to-end ELT pipelines (Fivetran, SQL, Pandas, R tidyverse) to ingest and transform marketing data (Google Ads, Facebook Ads, Google Analytics, Google Trends).\n- Designed and maintained cloud-based architectures (VMs, Cloud Run, GCS, Artifact Registry, BigQuery).\n- Managed PostgreSQL and BigQuery databases, ensuring data reliability and accessibility for analytics.\n\n\n\n\n\n\nAFL Game Prediction Model\nBuilt a hierarchical Bayesian model to forecast AFL outcomes with 82% out-of-sample accuracy. Designed an end-to-end ELT pipeline for ingesting, cleaning, and transforming match and player data for real-time updates.\nProperty Price Predictor\nDeveloped machine learning models (XGBoost, SVM) to predict Sydney property prices. Pipeline included data collection, preprocessing, postcode enrichment via APIs, and suburb ranking analysis. Evaluated performance using RMSE and MAPE.\n\nüëâ More details and source code available on my GitHub.\n\n\n\n\nBachelor of Science (Applied Mathematics Major)\nUniversity of Melbourne ‚Äî Graduated Nov 2023\n- Mathematics & Applied Analysis: Multivariate calculus, differential equations, complex analysis; applied problem-solving with R and MATLAB.\n- Data Science & Statistics: Proficient in Python for analysis/visualization, foundational ML algorithms, probability, estimation, hypothesis testing, GLMs.\n- Programming & Algorithms: Strong foundation in data structures and algorithms (binary trees, hash tables, asymptotic complexity).\nHigher School Certificate\nXavier High School ‚Äî Graduated Nov 2020\n- ATAR 98.35 with excellent scores in Physics, Mathematics Extension 1 & 2.\n\n\n\n\nLanguages & Tools: Python, R, SQL, Pandas, NumPy, scikit-learn, PyMC, PyTensor, ArviZ, Matplotlib, Seaborn\nPlatforms & APIs: OpenAI API, Flask\nDatabases: PostgreSQL, BigQuery\nDevOps & Cloud: Docker, Cloud Run, Google Cloud Platform, GitHub Actions\n\n\n\n\n\nGoogle Cloud Computing Foundations Certificate"
  },
  {
    "objectID": "CV.html#summary",
    "href": "CV.html#summary",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "Data Scientist with expertise in Bayesian modeling, forecasting, ML pipelines, and cloud-based solutions.\nSkilled in delivering scalable models and data-driven insights across fast-paced startup and academic environments."
  },
  {
    "objectID": "CV.html#experience",
    "href": "CV.html#experience",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "Jan 2024 ‚Äì Present\nData Science\n- Designed and implemented a commercial Marketing Mix Model in Python (PyMC), applying Hierarchical Bayes with partial pooling, nonlinear regression, and advanced adstock/saturation dynamics. Achieved out-of-sample MAPE as low as 11%.\n- Engineered a scalable PCA framework in R to condense a library of 80,000+ macroeconomic variables into actionable inputs.\n- Developed robust time series forecasting pipelines (ARIMA, VAR, LSTM/RNN) with synthetic data generation to ensure model continuity despite data delays.\nMachine Learning Operations (MLOps)\n- Built and maintained API servers, fully containerized with Docker, and implemented CI/CD pipelines via GitHub Actions, reducing deployment time by 80%.\n- Containerized and deployed multiple ML model interfaces to Google Cloud Run for scalable production.\n- Created interactive model evaluation frameworks (Python + HTML), streamlining model selection and benchmarking.\nData Engineering / Other\n- Built and managed end-to-end ELT pipelines (Fivetran, SQL, Pandas, R tidyverse) to ingest and transform marketing data (Google Ads, Facebook Ads, Google Analytics, Google Trends).\n- Designed and maintained cloud-based architectures (VMs, Cloud Run, GCS, Artifact Registry, BigQuery).\n- Managed PostgreSQL and BigQuery databases, ensuring data reliability and accessibility for analytics."
  },
  {
    "objectID": "CV.html#personal-projects",
    "href": "CV.html#personal-projects",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "AFL Game Prediction Model\nBuilt a hierarchical Bayesian model to forecast AFL outcomes with 82% out-of-sample accuracy. Designed an end-to-end ELT pipeline for ingesting, cleaning, and transforming match and player data for real-time updates.\nProperty Price Predictor\nDeveloped machine learning models (XGBoost, SVM) to predict Sydney property prices. Pipeline included data collection, preprocessing, postcode enrichment via APIs, and suburb ranking analysis. Evaluated performance using RMSE and MAPE.\n\nüëâ More details and source code available on my GitHub."
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "Bachelor of Science (Applied Mathematics Major)\nUniversity of Melbourne ‚Äî Graduated Nov 2023\n- Mathematics & Applied Analysis: Multivariate calculus, differential equations, complex analysis; applied problem-solving with R and MATLAB.\n- Data Science & Statistics: Proficient in Python for analysis/visualization, foundational ML algorithms, probability, estimation, hypothesis testing, GLMs.\n- Programming & Algorithms: Strong foundation in data structures and algorithms (binary trees, hash tables, asymptotic complexity).\nHigher School Certificate\nXavier High School ‚Äî Graduated Nov 2020\n- ATAR 98.35 with excellent scores in Physics, Mathematics Extension 1 & 2."
  },
  {
    "objectID": "CV.html#tech-stack",
    "href": "CV.html#tech-stack",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "Languages & Tools: Python, R, SQL, Pandas, NumPy, scikit-learn, PyMC, PyTensor, ArviZ, Matplotlib, Seaborn\nPlatforms & APIs: OpenAI API, Flask\nDatabases: PostgreSQL, BigQuery\nDevOps & Cloud: Docker, Cloud Run, Google Cloud Platform, GitHub Actions"
  },
  {
    "objectID": "CV.html#certifications",
    "href": "CV.html#certifications",
    "title": "Resume - Jed Toner",
    "section": "",
    "text": "Google Cloud Computing Foundations Certificate"
  },
  {
    "objectID": "blog/afl.html",
    "href": "blog/afl.html",
    "title": "AFL Prediction Model",
    "section": "",
    "text": "The following is a hierarchical bayesian model for AFL games, this model will be trained on the first 17 rounds of an AFL season, and tested on the final 6/7 rounds of the season\n\n## Libraries used for model and analysis\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pytensor.tensor as pt\nimport arviz as az\nimport matplotlib.pyplot as plt\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nStep 1: Obtaining the data\nThe data is obtained from Kaggle: https://www.kaggle.com/datasets/stoney71/aflstats/data?select=stats.csv\nSome preprocessing will need to be done to ensure it will work with the pymc model\n\ngames = pd.read_csv('data/games.csv')\ngames.head()\n\n\n\n\n\n\n\n\nGameId\nYear\nRound\nDate\nMaxTemp\nMinTemp\nRainfall\nVenue\nStartTime\nAttendance\n...\nHomeTeamScoreHT\nHomeTeamScore3QT\nHomeTeamScoreFT\nHomeTeamScore\nAwayTeam\nAwayTeamScoreQT\nAwayTeamScoreHT\nAwayTeamScore3QT\nAwayTeamScoreFT\nAwayTeamScore\n\n\n\n\n0\n2012R0101\n2012\nR1\n2012-03-24\n24.0\n12.2\n0.0\nStadium Australia\n7:20 PM\n38,203\n...\n3.3\n3.4\n5.70\n37\nSydney\n4.1\n8.4\n13.80\n14.16\n100\n\n\n1\n2012R0102\n2012\nR1\n2012-03-29\n25.7\n9.7\n0.0\nM.C.G.\n7:45 PM\n78,285\n...\n5.6\n10.7\n12.90\n81\nCarlton\n3.2\n8.7\n11.13\n18.17\n125\n\n\n2\n2012R0103\n2012\nR1\n2012-03-30\n27.4\n9.7\n0.0\nM.C.G.\n7:50 PM\n78,466\n...\n10.6\n14.1\n20.17\n137\nCollingwood\n2.7\n7.9\n12.16\n16.19\n115\n\n\n3\n2012R0104\n2012\nR1\n2012-03-31\n29.1\n15.1\n0.6\nM.C.G.\n1:45 PM\n33,473\n...\n7.4\n8.8\n11.12\n78\nBrisbane Lions\n1.4\n7.8\n13.13\n17.17\n119\n\n\n4\n2012R0105\n2012\nR1\n2012-03-31\n28.2\n19.7\n0.0\nCarrara\n3:45 PM\n12,790\n...\n5.3\n8.6\n10.80\n68\nAdelaide\n7.8\n11.1\n15.16\n19.23\n137\n\n\n\n\n5 rows √ó 22 columns\n\n\n\nWe will only keep the columns we need to avoid clutter\n\ngame_cols_to_keep = ['Year', 'Round', 'HomeTeam', 'HomeTeamScore', 'AwayTeam', 'AwayTeamScore']\ngame_df = games[game_cols_to_keep]\ngame_df.head()\n\n\n\n\n\n\n\n\nYear\nRound\nHomeTeam\nHomeTeamScore\nAwayTeam\nAwayTeamScore\n\n\n\n\n0\n2012\nR1\nGreater Western Sydney\n37\nSydney\n100\n\n\n1\n2012\nR1\nRichmond\n81\nCarlton\n125\n\n\n2\n2012\nR1\nHawthorn\n137\nCollingwood\n115\n\n\n3\n2012\nR1\nMelbourne\n78\nBrisbane Lions\n119\n\n\n4\n2012\nR1\nGold Coast\n68\nAdelaide\n137\n\n\n\n\n\n\n\n\nplayer_statistics = pd.read_csv('data/stats.csv')\nplayer_statistics.head().columns\n\nIndex(['GameId', 'Team', 'Year', 'Round', 'PlayerId', 'DisplayName',\n       'GameNumber', 'Disposals', 'Kicks', 'Marks', 'Handballs', 'Goals',\n       'Behinds', 'HitOuts', 'Tackles', 'Rebounds', 'Inside50s', 'Clearances',\n       'Clangers', 'Frees', 'FreesAgainst', 'BrownlowVotes',\n       'ContestedPossessions', 'UncontestedPossessions', 'ContestedMarks',\n       'MarksInside50', 'OnePercenters', 'Bounces', 'GoalAssists', '%Played',\n       'Subs'],\n      dtype='object')\n\n\nThe model will not analyse individual player performances, we just want team averages for each year to form a basis for ‚Äòattack‚Äô and ‚Äòdefense‚Äô latent variables. Goals, goal assists and behinds will correlate too strongly with the game scores and will not provide any extra information for the model so they will be dropped. Disposals will be dropped as they are a linear combination of kicks and handballs. Variables %Played, Brownlow Votes and Subs will not be used either. These stats will be sorted into offensive and defensive metrics which is how they will be read into the model\n\ncols_to_keep = ['Team', 'Year', 'Round',\n       'Disposals', 'Kicks', 'Marks', 'Handballs',\n       'HitOuts', 'Tackles', 'Rebounds', 'Inside50s', 'Clearances',\n       'Clangers', 'Frees', 'FreesAgainst', 'ContestedPossessions', 'UncontestedPossessions', 'ContestedMarks',\n       'MarksInside50', 'OnePercenters', 'Bounces']\n\nteam_stats_long = player_statistics[cols_to_keep]\nrounds_to_keep = ['R' + str(i) for i in range(1,25)] #removing finals for simplicity\nteam_stats_long = team_stats_long[team_stats_long['Round'].isin(rounds_to_keep)]\nteam_stats_long.drop('Round', axis=1, inplace=True)\n\nteam_stats = team_stats_long.groupby(['Team', 'Year']).sum().sort_values(['Year', 'Team']).reset_index()\nteam_stats.head()\n\n\n\n\n\n\n\n\nTeam\nYear\nDisposals\nKicks\nMarks\nHandballs\nHitOuts\nTackles\nRebounds\nInside50s\nClearances\nClangers\nFrees\nFreesAgainst\nContestedPossessions\nUncontestedPossessions\nContestedMarks\nMarksInside50\nOnePercenters\nBounces\n\n\n\n\n0\nAdelaide\n2012\n7709\n4793\n1969\n2916\n981\n1304\n736\n1163\n911\n944\n373\n397\n3316\n4341\n283\n304\n1032\n129\n\n\n1\nBrisbane Lions\n2012\n7716\n4403\n1890\n3313\n749\n1393\n817\n1048\n808\n1097\n398\n446\n3086\n4595\n204\n218\n1230\n116\n\n\n2\nCarlton\n2012\n7853\n4625\n1975\n3228\n937\n1520\n651\n1165\n867\n986\n398\n419\n3131\n4648\n230\n294\n1128\n272\n\n\n3\nCollingwood\n2012\n8198\n4823\n2017\n3375\n785\n1405\n784\n1159\n842\n946\n366\n365\n3218\n4933\n270\n271\n1088\n265\n\n\n4\nEssendon\n2012\n7765\n4666\n2003\n3099\n942\n1345\n774\n1225\n798\n1027\n418\n403\n3147\n4600\n261\n252\n1094\n168\n\n\n\n\n\n\n\n\nteam_stats.drop(['Team', 'Year'], axis = 1).corr()\n\n\n\n\n\n\n\n\nDisposals\nKicks\nMarks\nHandballs\nHitOuts\nTackles\nRebounds\nInside50s\nClearances\nClangers\nFrees\nFreesAgainst\nContestedPossessions\nUncontestedPossessions\nContestedMarks\nMarksInside50\nOnePercenters\nBounces\n\n\n\n\nDisposals\n1.000000\n0.910351\n0.839811\n0.903049\n0.571414\n0.789255\n0.610808\n0.862372\n0.807113\n0.582055\n0.680007\n0.596433\n0.901822\n0.969491\n0.637916\n0.727281\n0.665613\n0.337098\n\n\nKicks\n0.910351\n1.000000\n0.911171\n0.644332\n0.536117\n0.665195\n0.705629\n0.862463\n0.815880\n0.675174\n0.701558\n0.627580\n0.866330\n0.836495\n0.694894\n0.729139\n0.719775\n0.266493\n\n\nMarks\n0.839811\n0.911171\n1.000000\n0.606178\n0.483379\n0.552818\n0.579840\n0.695572\n0.652370\n0.542559\n0.608759\n0.525762\n0.675559\n0.849695\n0.640515\n0.664675\n0.514708\n0.274344\n\n\nHandballs\n0.903049\n0.644332\n0.606178\n1.000000\n0.499480\n0.768061\n0.396336\n0.698426\n0.644660\n0.374812\n0.528437\n0.450781\n0.767312\n0.923328\n0.457571\n0.587168\n0.482929\n0.346332\n\n\nHitOuts\n0.571414\n0.536117\n0.483379\n0.499480\n1.000000\n0.600626\n0.285175\n0.512277\n0.708788\n0.204307\n0.452939\n0.359242\n0.634800\n0.503586\n0.488756\n0.422544\n0.311608\n0.214313\n\n\nTackles\n0.789255\n0.665195\n0.552818\n0.768061\n0.600626\n1.000000\n0.424493\n0.709848\n0.755650\n0.357743\n0.545966\n0.552527\n0.825077\n0.720601\n0.511500\n0.590045\n0.587356\n0.351427\n\n\nRebounds\n0.610808\n0.705629\n0.579840\n0.396336\n0.285175\n0.424493\n1.000000\n0.483696\n0.531633\n0.841321\n0.583252\n0.646899\n0.583790\n0.525661\n0.421032\n0.278173\n0.582944\n0.027090\n\n\nInside50s\n0.862372\n0.862463\n0.695572\n0.698426\n0.512277\n0.709848\n0.483696\n1.000000\n0.797782\n0.567436\n0.623790\n0.576188\n0.891526\n0.768696\n0.675053\n0.862492\n0.699808\n0.292668\n\n\nClearances\n0.807113\n0.815880\n0.652370\n0.644660\n0.708788\n0.755650\n0.531633\n0.797782\n1.000000\n0.435312\n0.626129\n0.558520\n0.877533\n0.698812\n0.559309\n0.633563\n0.673268\n0.357811\n\n\nClangers\n0.582055\n0.675174\n0.542559\n0.374812\n0.204307\n0.357743\n0.841321\n0.567436\n0.435312\n1.000000\n0.614480\n0.735799\n0.557747\n0.504727\n0.412057\n0.387999\n0.579595\n-0.118877\n\n\nFrees\n0.680007\n0.701558\n0.608759\n0.528437\n0.452939\n0.545966\n0.583252\n0.623790\n0.626129\n0.614480\n1.000000\n0.703283\n0.712946\n0.603252\n0.580819\n0.463295\n0.545038\n0.106843\n\n\nFreesAgainst\n0.596433\n0.627580\n0.525762\n0.450781\n0.359242\n0.552527\n0.646899\n0.576188\n0.558520\n0.735799\n0.703283\n1.000000\n0.639463\n0.516929\n0.477228\n0.418754\n0.575669\n0.123125\n\n\nContestedPossessions\n0.901822\n0.866330\n0.675559\n0.767312\n0.634800\n0.825077\n0.583790\n0.891526\n0.877533\n0.557747\n0.712946\n0.639463\n1.000000\n0.778332\n0.714830\n0.709877\n0.723629\n0.304789\n\n\nUncontestedPossessions\n0.969491\n0.836495\n0.849695\n0.923328\n0.503586\n0.720601\n0.525661\n0.768696\n0.698812\n0.504727\n0.603252\n0.516929\n0.778332\n1.000000\n0.546597\n0.677963\n0.562319\n0.339456\n\n\nContestedMarks\n0.637916\n0.694894\n0.640515\n0.457571\n0.488756\n0.511500\n0.421032\n0.675053\n0.559309\n0.412057\n0.580819\n0.477228\n0.714830\n0.546597\n1.000000\n0.671974\n0.416370\n0.155026\n\n\nMarksInside50\n0.727281\n0.729139\n0.664675\n0.587168\n0.422544\n0.590045\n0.278173\n0.862492\n0.633563\n0.387999\n0.463295\n0.418754\n0.709877\n0.677963\n0.671974\n1.000000\n0.472556\n0.295628\n\n\nOnePercenters\n0.665613\n0.719775\n0.514708\n0.482929\n0.311608\n0.587356\n0.582944\n0.699808\n0.673268\n0.579595\n0.545038\n0.575669\n0.723629\n0.562319\n0.416370\n0.472556\n1.000000\n0.218094\n\n\nBounces\n0.337098\n0.266493\n0.274344\n0.346332\n0.214313\n0.351427\n0.027090\n0.292668\n0.357811\n-0.118877\n0.106843\n0.123125\n0.304789\n0.339456\n0.155026\n0.295628\n0.218094\n1.000000\n\n\n\n\n\n\n\nClearly a lot of the stats are highly correlated which will justify the use of PCA on the dataset\n\ndef pca(team_statistics_dataset):\n    \n    teams = team_statistics_dataset['Team']\n    years = team_statistics_dataset['Year']\n\n    team_stats = team_statistics_dataset.drop(['Team', 'Year'], axis = 1)\n\n    # Standardize the data\n    scaler = StandardScaler()\n    team_stats_scaled = scaler.fit_transform(team_stats)\n\n    # Fit the PCA model\n    pca = PCA(n_components = 0.95) # To retain 95% of the variance\n    new_data = pca.fit_transform(team_stats_scaled)\n\n    # Format the pca data into a dataframe\n\n    pca_data = pd.DataFrame(new_data, columns = ['PC' + str(i) for i in range(1, new_data.shape[1] + 1)])\n    pca_data['Team'] = teams\n    pca_data['Year'] = years\n    final_columns = ['Team', 'Year'] + ['PC' + str(i) for i in range(1, new_data.shape[1] + 1)]\n    pca_data = pca_data[final_columns]\n       \n    return pca_data\n\nPCA_stats = pca(team_stats)\nPCA_stats\n\n\n\n\n\n\n\n\nTeam\nYear\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\n\n\n\n\n0\nAdelaide\n2012\n0.658792\n-0.984249\n2.014072\n0.701787\n-1.409568\n-0.106361\n-0.790861\n-0.232091\n0.094558\n\n\n1\nBrisbane Lions\n2012\n-0.074597\n1.021397\n-1.181794\n0.554890\n-0.257283\n-0.932035\n-0.211187\n0.481955\n-0.058517\n\n\n2\nCarlton\n2012\n0.931581\n-1.997261\n-0.396465\n0.516284\n-1.557807\n-0.147811\n0.164286\n-0.080944\n-0.745708\n\n\n3\nCollingwood\n2012\n0.936581\n-1.732937\n-0.196525\n-0.739634\n-1.171677\n0.274340\n-0.534968\n0.162109\n1.026016\n\n\n4\nEssendon\n2012\n0.687605\n-0.341206\n0.705276\n0.461726\n-0.934017\n0.173532\n-0.230057\n0.392014\n0.089700\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n211\nRichmond\n2023\n2.337820\n1.487335\n-0.185844\n-0.374654\n-0.952431\n-0.078276\n-1.424848\n-1.872549\n0.505720\n\n\n212\nSt Kilda\n2023\n2.673075\n1.001132\n-0.171502\n-1.805777\n1.210700\n0.351236\n-0.825929\n0.307152\n0.317417\n\n\n213\nSydney\n2023\n2.363447\n1.790445\n-0.776916\n-0.237490\n-0.693070\n-0.835622\n-0.132525\n-0.307637\n0.319752\n\n\n214\nWest Coast\n2023\n-0.372962\n1.529812\n-1.414003\n-0.161040\n1.629578\n0.757098\n-0.364209\n-0.715334\n-0.339356\n\n\n215\nWestern Bulldogs\n2023\n2.492283\n0.072285\n-0.395208\n0.147377\n0.104553\n-1.350135\n-1.755657\n-0.165438\n-0.122689\n\n\n\n\n216 rows √ó 11 columns\n\n\n\nWe will create a data class to organise all constant data for the pymc model, including variables that will inform the parameters of our prior distributions. The model will only analyse one year at a time so the season will be a parameter for the data class\n\nclass Data:\n    def __init__(self, game_df, team_stats_df, season, rounds = 23):\n        rounds = ['R' + str(i) for i in range(1, rounds + 1)]\n        self.game_df = game_df.loc[(game_df['Year'] == season) & (game_df['Round'].isin(rounds))].reset_index(drop=True)\n        self._add_winner_column()\n        self.team_stats_df = team_stats_df.loc[team_stats_df['Year'] == season].reset_index(drop = True)\n        self._infer_statistics()\n        self.coords = {\"team\": self.teams,\n                       \"metric\": self.team_stats_df.drop(['Team', 'Year'], axis = 1).columns,\n                       \"match\": self.game_df.index}\n        self._priors()\n\n    def _infer_statistics(self):\n        self.home_idx, self.teams = pd.factorize(self.game_df[\"HomeTeam\"], sort=True)\n        self.away_idx, _ = pd.factorize(self.game_df[\"AwayTeam\"], sort=True)\n\n    def _priors(self):\n        ratio = self.game_df[\"HomeTeamScore\"] / self.game_df[\"AwayTeamScore\"]\n        self.avg_home_point_advantage = ratio.mean()\n        self.home_point_advantage_std = ratio.std()\n        all_scores = np.array(pd.concat([self.game_df[\"HomeTeamScore\"], self.game_df[\"AwayTeamScore\"]]))\n        self.avg_score = np.mean(all_scores)\n        self.score_std = np.std(all_scores)\n\n    def _add_winner_column(self):\n        self.game_df[\"Winner\"] = self.game_df.apply(lambda x: x[\"HomeTeam\"] if x[\"HomeTeamScore\"] &gt; x[\"AwayTeamScore\"] else x[\"AwayTeam\"], axis=1)\n\ndata = Data(game_df, PCA_stats, 2017, rounds = 23)\n    \n\nThe pymc model.\nThe model will attempt to learn latent attack and defense paramters for each team. These parameters will be formed by a linear regression of the team statistics variables, as well as a team specific intercept term to capture information that leads to a teams score that is not captured by the variables in team statistics.\nThe scoring is modelled by a normal distribution rather than a typical poisson distribution to better account for the higher spread in points scored\n\ndef create_model(data):\n\n    with pm.Model(coords = data.coords) as model:\n\n        #constant data\n        home_team = pm.ConstantData('home_team', data.home_idx, dims = \"match\")\n        away_team = pm.ConstantData('away_team', data.away_idx, dims = \"match\")\n\n        team_statistics = pm.ConstantData('team_statistics', data.team_stats_df.drop(['Team', 'Year'], axis = 1).values, dims = [\"team\", \"metric\"])\n\n        #global model parameters\n        home_uncentered = pm.Normal('home_uncentered', mu = 0, sigma = 1)\n        home = pm.Deterministic('home', home_uncentered * np.log(data.home_point_advantage_std) + np.log(data.avg_home_point_advantage))\n\n        scoring_intercept_uncentered = pm.Normal('scoring_intercept_uncentered', mu = 0, sigma = 1)\n        scoring_intercept = pm.Deterministic('scoring_intercept', scoring_intercept_uncentered * np.log(data.score_std) + np.log(data.avg_score))\n\n        #team-specific model parameters\n        att_beta = pm.ZeroSumNormal('att_beta', sigma = 0.05, dims = \"metric\")\n        offensive_intercept = pm.Normal('offensive_intercept', mu = 0, sigma = 1, dims=\"team\")\n        atts_star = pm.Deterministic('atts_star', pm.math.dot(att_beta, team_statistics.T) + offensive_intercept, dims = \"team\")\n\n        def_beta = pm.ZeroSumNormal('def_beta', sigma = 0.05, dims = \"metric\")\n        defensive_intercept = pm.Normal('defensive_intercept', mu = 0, sigma = 1, dims=\"team\")\n        defs_star = pm.Deterministic('defs_star', pm.math.dot(def_beta, team_statistics.T) + defensive_intercept, dims = \"team\")\n\n        atts = pm.Deterministic('atts', atts_star - pt.mean(atts_star), dims = \"team\")\n        defs = pm.Deterministic('defs', defs_star - pt.mean(defs_star), dims = \"team\")\n\n        #scoring\n        home_mu = pt.exp(scoring_intercept + home + atts[home_team] - defs[away_team])\n        away_mu = pt.exp(scoring_intercept + atts[away_team] - defs[home_team])\n\n        sigma_uncentered = pm.Exponential(\"sigma_uncentered\", scale = 1, dims = \"team\")\n        sigma = pm.Deterministic(\"sigma\", sigma_uncentered * np.log(data.score_std), dims = \"team\")\n\n        home_points = pm.Normal(\"home_points\",\n                                mu = home_mu,\n                                sigma = sigma[home_team],\n                                observed = data.game_df[\"HomeTeamScore\"],\n                                dims = \"match\")\n        \n        away_points = pm.Normal(\"away_points\",\n                                mu = away_mu,\n                                sigma = sigma[away_team],\n                                observed = data.game_df[\"AwayTeamScore\"],\n                                dims = \"match\")\n        \n        trace = pm.sample(draws = 1000,\n                        tune = 5000,\n                        chains = 4,\n                        cores = 4,)\n        \n    return trace\n\ntrace = create_model(data)\naz.to_netcdf(trace, 'trace.nc') #save trace for later use\n\n/Users/jedtoner1/Desktop/Developer/AFLSeason/AFL/lib/python3.12/site-packages/pymc/data.py:274: FutureWarning: ConstantData is deprecated. All Data variables are now mutable. Use Data instead.\n  warnings.warn(\n/Users/jedtoner1/Desktop/Developer/AFLSeason/AFL/lib/python3.12/site-packages/pymc/data.py:274: FutureWarning: ConstantData is deprecated. All Data variables are now mutable. Use Data instead.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [home_uncentered, scoring_intercept_uncentered, att_beta, offensive_intercept, def_beta, defensive_intercept, sigma_uncentered]\n\n\n/Users/jedtoner1/Desktop/Developer/AFLSeason/AFL/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \ninstall \"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n\n\n\nSampling 4 chains for 5_000 tune and 1_000 draw iterations (20_000 + 4_000 draws total) took 53 seconds.\n\n\n'trace.nc'\n\n\nWe will look at plots and diagnostics to see if our model has converged\n\ntrace = az.from_netcdf('trace.nc') #load model trace\n\n\naz.plot_trace(trace, var_names = [\"home\", \"scoring_intercept\"]);\n\n\n\n\n\n\n\n\n\naz.summary(trace, var_names = [\"atts\", \"defs\"], kind = \"diagnostics\")\n\n\n\n\n\n\n\n\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\natts[Adelaide]\n0.001\n0.001\n4116.0\n3077.0\n1.0\n\n\natts[Brisbane Lions]\n0.001\n0.000\n4238.0\n3662.0\n1.0\n\n\natts[Carlton]\n0.001\n0.000\n3784.0\n3666.0\n1.0\n\n\natts[Collingwood]\n0.001\n0.000\n4000.0\n3781.0\n1.0\n\n\natts[Essendon]\n0.001\n0.001\n3959.0\n3120.0\n1.0\n\n\natts[Fremantle]\n0.001\n0.001\n3855.0\n2945.0\n1.0\n\n\natts[Geelong]\n0.001\n0.000\n4149.0\n3120.0\n1.0\n\n\natts[Gold Coast]\n0.001\n0.001\n3879.0\n2968.0\n1.0\n\n\natts[Greater Western Sydney]\n0.001\n0.000\n4075.0\n3086.0\n1.0\n\n\natts[Hawthorn]\n0.001\n0.001\n4026.0\n3041.0\n1.0\n\n\natts[Melbourne]\n0.001\n0.001\n4067.0\n3369.0\n1.0\n\n\natts[North Melbourne]\n0.001\n0.001\n4162.0\n3181.0\n1.0\n\n\natts[Port Adelaide]\n0.001\n0.001\n4197.0\n3354.0\n1.0\n\n\natts[Richmond]\n0.001\n0.001\n4004.0\n3142.0\n1.0\n\n\natts[St Kilda]\n0.001\n0.001\n4029.0\n2895.0\n1.0\n\n\natts[Sydney]\n0.001\n0.000\n4207.0\n3395.0\n1.0\n\n\natts[West Coast]\n0.001\n0.000\n4271.0\n3575.0\n1.0\n\n\natts[Western Bulldogs]\n0.001\n0.001\n4169.0\n3528.0\n1.0\n\n\ndefs[Adelaide]\n0.001\n0.001\n4017.0\n3264.0\n1.0\n\n\ndefs[Brisbane Lions]\n0.001\n0.000\n4118.0\n3178.0\n1.0\n\n\ndefs[Carlton]\n0.001\n0.001\n4086.0\n3370.0\n1.0\n\n\ndefs[Collingwood]\n0.001\n0.001\n4281.0\n3189.0\n1.0\n\n\ndefs[Essendon]\n0.001\n0.001\n4123.0\n3480.0\n1.0\n\n\ndefs[Fremantle]\n0.001\n0.000\n4102.0\n3645.0\n1.0\n\n\ndefs[Geelong]\n0.001\n0.001\n4167.0\n3332.0\n1.0\n\n\ndefs[Gold Coast]\n0.001\n0.000\n3927.0\n3715.0\n1.0\n\n\ndefs[Greater Western Sydney]\n0.001\n0.001\n4241.0\n3154.0\n1.0\n\n\ndefs[Hawthorn]\n0.001\n0.001\n4313.0\n3402.0\n1.0\n\n\ndefs[Melbourne]\n0.001\n0.001\n3605.0\n3504.0\n1.0\n\n\ndefs[North Melbourne]\n0.001\n0.000\n4175.0\n3479.0\n1.0\n\n\ndefs[Port Adelaide]\n0.001\n0.001\n4058.0\n3383.0\n1.0\n\n\ndefs[Richmond]\n0.001\n0.001\n4229.0\n3594.0\n1.0\n\n\ndefs[St Kilda]\n0.001\n0.001\n4420.0\n3620.0\n1.0\n\n\ndefs[Sydney]\n0.001\n0.001\n4127.0\n3138.0\n1.0\n\n\ndefs[West Coast]\n0.001\n0.001\n3775.0\n2900.0\n1.0\n\n\ndefs[Western Bulldogs]\n0.001\n0.001\n4031.0\n3241.0\n1.0\n\n\n\n\n\n\n\nOur model has converged and Rhat looks good.\nWe will now inspect the parameters to see if they match up with industry knowledge. The 2017 season saw Adelaide, Richmond, Geelong do well and Carlton, Gold Coast and Brisbane do poorly\n\ntrace.posterior[\"atts\"].mean(dim = [\"chain\", \"draw\"]).to_pandas().sort_values()\n\nteam\nCarlton                  -0.205138\nFremantle                -0.193383\nGold Coast               -0.112112\nWestern Bulldogs         -0.057072\nHawthorn                 -0.051424\nBrisbane Lions           -0.020016\nSt Kilda                 -0.013627\nCollingwood              -0.000820\nRichmond                  0.000670\nNorth Melbourne           0.005767\nWest Coast                0.007059\nMelbourne                 0.024026\nEssendon                  0.060009\nSydney                    0.070289\nGreater Western Sydney    0.087530\nPort Adelaide             0.089931\nGeelong                   0.093058\nAdelaide                  0.215254\ndtype: float64\n\n\n\ntrace.posterior[\"defs\"].mean(dim = [\"chain\", \"draw\"]).to_pandas().sort_values()\n\nteam\nBrisbane Lions           -0.261642\nGold Coast               -0.182726\nNorth Melbourne          -0.161610\nFremantle                -0.086852\nCarlton                  -0.034198\nEssendon                 -0.024289\nSt Kilda                 -0.017908\nHawthorn                 -0.010141\nCollingwood               0.002900\nMelbourne                 0.005190\nWestern Bulldogs          0.033926\nWest Coast                0.058964\nGeelong                   0.087093\nGreater Western Sydney    0.088482\nAdelaide                  0.091281\nSydney                    0.132509\nRichmond                  0.135809\nPort Adelaide             0.143211\ndtype: float64\n\n\nSo the model parameters recover what we already knew about the 2017 season. Lets plot the latent variables for each team\n\ntrace_hdi = az.hdi(trace)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.scatter(data.teams, trace.posterior[\"atts\"].median(dim=(\"chain\", \"draw\")), color=\"C0\", alpha=1, s=100)\nax.vlines(\n    data.teams,\n    trace_hdi[\"atts\"].sel({\"hdi\": \"lower\"}),\n    trace_hdi[\"atts\"].sel({\"hdi\": \"higher\"}),\n    alpha=0.6,\n    lw=5,\n    color=\"C0\",\n)\nax.set_xticks(range(len(data.teams)))\nax.set_xticklabels(data.teams, rotation=60);\nax.set_xlabel(\"Teams\")\nax.set_ylabel(\"Posterior Attack Strength\")\nax.set_title(\"HDI of Team-wise Attack Strength\");\n\n\n\n\n\n\n\n\n\ntrace_hdi = az.hdi(trace)\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.scatter(data.teams, trace.posterior[\"defs\"].median(dim=(\"chain\", \"draw\")), color=\"C0\", alpha=1, s=100)\nax.vlines(\n    data.teams,\n    trace_hdi[\"defs\"].sel({\"hdi\": \"lower\"}),\n    trace_hdi[\"defs\"].sel({\"hdi\": \"higher\"}),\n    alpha=0.6,\n    lw=5,\n    color=\"C0\",\n)\nax.set_xticks(range(len(data.teams)))\nax.set_xticklabels(data.teams, rotation=60);\nax.set_xlabel(\"Teams\")\nax.set_ylabel(\"Posterior Attack Strength\")\nax.set_title(\"HDI of Team-wise Attack Strength\");\n\n\n\n\n\n\n\n\nWe will build functions to predict the outcome of games and probability of winning\n\ndef predict_game(home_team, away_team, trace, variance = True):\n    \n    post = trace.posterior\n\n    home = post[\"home\"].mean()\n    scoring_intercept = post[\"scoring_intercept\"].mean()\n    atts = post[\"atts\"].mean(dim = [\"chain\", \"draw\"])\n    defs = post[\"defs\"].mean(dim = [\"chain\", \"draw\"])\n\n    home_mu = np.exp(scoring_intercept + home + atts.sel(team = home_team) - defs.sel(team = away_team)).values\n    away_mu = np.exp(scoring_intercept + atts.sel(team = away_team) - defs.sel(team = home_team)).values\n\n    if variance:\n        sigma = post[\"sigma\"].mean(dim = [\"chain\", \"draw\"]).sel(team = home_team)\n        home_points = np.random.normal(home_mu, sigma)\n        sigma = post[\"sigma\"].mean(dim = [\"chain\", \"draw\"]).sel(team = away_team)\n        away_points = np.random.normal(away_mu, sigma)\n    else:\n        home_points = home_mu\n        away_points = away_mu\n\n    return (home_team, home_points, away_team, away_points)\n\ndef predict_winner(home_team, away_team, trace):\n    home_team, home_points, away_team, away_points = predict_game(home_team, away_team, trace, variance = False)\n    return home_team if home_points &gt; away_points else away_team\n\n\npredict_game(\"Richmond\", \"Adelaide\", trace, variance=False)\n\n('Richmond', array(82.89362002), 'Adelaide', array(92.0250401))\n\n\nLets test the in sample accuracy for predicting the winner of games\n\ntrace = az.from_netcdf('trace.nc') #load model trace\n\n\ngames_2017 = games.loc[games[\"Year\"] == 2017].reset_index(drop=True)\ngames_2017 = games_2017[~games_2017[\"Round\"].str.contains(\"F\")] #remove finals\ngames_2017['Winner'] = games_2017.apply(lambda x: x[\"HomeTeam\"] if x[\"HomeTeamScore\"] &gt; x[\"AwayTeamScore\"] else x[\"AwayTeam\"], axis=1)\ngames_2017['Predicted Winner'] = games_2017.apply(lambda x: predict_winner(x[\"HomeTeam\"], x[\"AwayTeam\"], trace), axis=1)\naccuracy = (games_2017[\"Winner\"] == games_2017[\"Predicted Winner\"]).mean()\nprint(accuracy)\n\n0.702020202020202\n\n\nSo we have a 70% in sample accuracy. Lets retrain a model on 2022 data to test the out of sample accuracy for the last 7 rounds of the season\nWe will have to reconstruct the team stats principal components as we can only take into account the first 17 rounds of the season, an assumption used here is that the team stats for the first 17 rounds can be scaled up by an appropriate factor to get the team stats for the whole 23 round season\n\n# Load player statistics\nplayer_statistics = pd.read_csv('data/stats.csv')\nplayer_statistics = player_statistics.loc[player_statistics['Year'] &lt;= 2022].reset_index(drop=True)\n\n# Columns to keep\ncols_to_keep = ['Team', 'Year', 'Round',\n       'Disposals', 'Kicks', 'Marks', 'Handballs',\n       'HitOuts', 'Tackles', 'Rebounds', 'Inside50s', 'Clearances',\n       'Clangers', 'Frees', 'FreesAgainst', 'ContestedPossessions', 'UncontestedPossessions', 'ContestedMarks',\n       'MarksInside50', 'OnePercenters', 'Bounces']\n\n# Filter the DataFrame to keep only the necessary columns\nteam_stats_long = player_statistics[cols_to_keep]\n\n# Define rounds to keep (excluding finals for simplicity)\nrounds_to_keep = ['R' + str(i) for i in range(1, 25)]  # R1 to R24\nfirst_17_rounds = ['R' + str(i) for i in range(1, 18)]  # R1 to R17\n\n# Filter the DataFrame to include only the specified rounds\nteam_stats_long = team_stats_long[team_stats_long['Round'].isin(rounds_to_keep)]\n\n# Drop rows where Year is 2022 and Round is 17 or before\nteam_stats_long = team_stats_long[~((team_stats_long['Year'] == 2022) & ~team_stats_long['Round'].isin(first_17_rounds))]\n\nteam_stats_long.drop('Round', axis=1, inplace=True)\n\nteam_stats = team_stats_long.groupby(['Team', 'Year']).sum().sort_values(['Year', 'Team']).reset_index()\n       \nstat_cols = team_stats.select_dtypes(include='number').columns.drop('Year')\nteam_stats[stat_cols] = team_stats[stat_cols].astype(float)\nteam_stats.loc[team_stats['Year'] == 2022, stat_cols] *= (23/17) # Scale the 2022 stats to 23 rounds\n\nPCA_stats = pca(team_stats)\n\ndata = Data(game_df, PCA_stats, 2022, rounds=17)\n\n\ntrace = create_model(data)\naz.to_netcdf(trace, 'trace_2022.nc') #save trace for later use\n\n/Users/jedtoner1/Desktop/Developer/AFLSeason/AFL/lib/python3.12/site-packages/pymc/data.py:274: FutureWarning: ConstantData is deprecated. All Data variables are now mutable. Use Data instead.\n  warnings.warn(\n/Users/jedtoner1/Desktop/Developer/AFLSeason/AFL/lib/python3.12/site-packages/pymc/data.py:274: FutureWarning: ConstantData is deprecated. All Data variables are now mutable. Use Data instead.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [home_uncentered, scoring_intercept_uncentered, att_beta, offensive_intercept, def_beta, defensive_intercept, sigma_uncentered]\n\n\n/Users/jedtoner1/Desktop/Developer/AFLSeason/AFL/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \ninstall \"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n\n\n\nSampling 4 chains for 5_000 tune and 1_000 draw iterations (20_000 + 4_000 draws total) took 47 seconds.\n\n\n'trace_2022.nc'\n\n\n\ntrace = az.from_netcdf('trace_2022.nc')\n\n\ngames_2022 = games.loc[games[\"Year\"] == 2022].reset_index(drop=True)\ngames_2022 = games_2022[~games_2022[\"Round\"].str.contains(\"F\")] #remove finals\ngames_2022['Winner'] = games_2022.apply(lambda x: x[\"HomeTeam\"] if x[\"HomeTeamScore\"] &gt; x[\"AwayTeamScore\"] else x[\"AwayTeam\"], axis=1)\ngames_2022['Predicted Winner'] = games_2022.apply(lambda x: predict_winner(x[\"HomeTeam\"], x[\"AwayTeam\"], trace), axis=1)\ntest_rounds = [f'R{i}' for i in range(18, 24)]\ntest_games = games_2022[games_2022['Round'].isin(test_rounds)]\naccuracy = (test_games[\"Winner\"] == test_games[\"Predicted Winner\"]).mean()\nprint(accuracy)\n\n0.8148148148148148\n\n\nSurprisingly greater than the in sample accuracy for 2017, this could be due to 2022 being a more predictable season. Nonetheless a good result\nPossible area for improvement:\n\nImplement form tracking - perhaps by time varying coefficients or a ‚Äòform‚Äô variable measuring effectiveness over past 5 games\nPlayer level modelling - quantifying the contribution of each player to the team. This could form the basis of models spanning over multiple years and tracking player movement\nImplementing variables such as weather, crowd impact and better handling of home advantage - perhaps specific to city or team\nTracking more advanced statistics that may have a higher impact on games, these might include meters gained, hard ball gets, broken tackles\nBetter handling/modelling of scoring sources - these include scoring from stoppage, kick ins and tunrover"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jed Toner - Data Scientist",
    "section": "",
    "text": "My experience lies in:\n\nBayesian hierarchical modeling\n\nCausal inference\n\nMachine learning\n\nI have worked on developing complex market mix models that incorporate various hierarchies and nonlinearities.\nI also have experience in machine learning operations (MLOps):\n\nProductionizing models by building API servers\n\nMaintaining CI/CD pipelines\n\nEnsuring reliable and efficient deployment of models\n\n\n\n\n\nAFL Prediction Model\n\nProperty Price Prediction\n\n\n\n\n\nüìß Email: jedto.13@gmail.com"
  },
  {
    "objectID": "index.html#github-projects",
    "href": "index.html#github-projects",
    "title": "Jed Toner - Data Scientist",
    "section": "",
    "text": "AFL Prediction Model\n\nProperty Price Prediction"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jed Toner - Data Scientist",
    "section": "",
    "text": "üìß Email: jedto.13@gmail.com"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\nWelcome to my blog!\nHere you‚Äôll find articles and posts on data science, Bayesian modeling, forecasting, machine learning pipelines, and personal projects.\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAFL Prediction Model\n\n\n\n\n\n\n\n\nSep 30, 2025\n\n\nJed Toner\n\n\n\n\n\nNo matching items"
  }
]